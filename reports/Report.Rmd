---
title: "CSC8631 Report"
author: "Antreas Kasiotis"
date: "07/11/2021"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir= normalizePath('..'))
```

```{r ProjectTemplate, include=FALSE}
library(ProjectTemplate)
load.project()

```

# Business Understanding

## Determine Business Objectives

### Background

This project concerns an online course. As part of this project, we seek to carry out learning analytics to measure and analyse the data that regularly get collected about the learners of this course. The purpose of this analysis is to understand and optimize learning and the environment in which it occurs.

### Business Objectives

1. Investigation into how the students engaged with the course. Which variables were responsible for their engagement with the course (resources used, steps completed, videos watched, etc.)? Looking at the changes that could be made (what worked well, what didn't). This could help improve the curriculum design.

2. Investigation into student performance. Understand which variables may have influenced their performance and progress (step quiz, grades, background characteristics, etc.). This could help better inform how to better support students.

3. Investigation into student background. Understand the target audience of the learners that participate in this course and identify the characteristics of those that were successful and those that were not.

## Assess Situation

### Resources

The resources that were provided for this project are a series of recorded log data (enrollments, arch-type survey responses, leaving survey response, weekly sentiment survey, team members, Quiz performance, step attendance, video coverage) for 7 different runs. In total, that is eight distinct data sources that could be used to draw inferences from regarding the business objectives.

### Requirements

The main requirement is to use some or all of the provided sets of data to create a data analysis pipeline for a set of reliable, quick and reproducible set of tools that will be used to derive information regarding my business questions.

### Assumptions and constraints

Multiple assumptions will have to be made as part of this project in terms of indicating the business objectives. Since I am not in contact with the company to discuss the aims and goals of the analysis with these data sets, I will have to determine some business objectives which I deem important and useful to investigate within the data sets.

The constraints face by the exploratory analysis are multiple. Firstly, the eight aforementioned data sources were not recorded for every run, in some runs only a small number of sources were recorded which limits our ability to effectively compare these runs in term of each source of recorded data. Another constraint is various data sets, there are entries with missing values for various variables. This is a problem because it means that the samples, we can extract for some variables are not really representative of the whole dataset.


## Determine data mining goals

### Goals

1. Learner Background:
- Finding the educational background and personal characteristics of the learners
- Finding if and how their background and characteristics affected their learning 

2. Learner Engagement:
- Finding methods to quantify the engagement of the students with the course.
- Finding where their engagement was attributed

3. Learner Performance:
- Finding how the students performed in this course
- Finding where their performance was attributed

### Success Criteria

The successfulness of my data mining project is dependent on the a variety of criteria, such as:

1. Having structured and clear business goals for what it is that I am trying to achieve from my data mining process.
2. Further dividing the business drivers down to straightforward hypotheses that can be addressed directly.
3. Thorough questioning of the reliability, completeness and relevance of the data sources used to avoid producing biased, unreliable or unimportant results.

## Project Plan

### Plan

My plan for this project is going to be based around the steps that formulate the CRISP-DM methodology. To begin with, I will look at all eight data sources methodically to try and understand what it is that lays within each set and how it can be used. I will discuss how the data composition and how the methods that they can be manipulated to best extract answers for my data mining goals. Furthermore, I look for issues with the data that need cleaning and reformatting, this will be done to verify its quality before moving on to the next steps. From there on, I will commence by picking out the data that is relevant to my data mining goals and work exclusively with that. I will clean that data, construct new records of it, merge it with other pre-existing data and reformat it wherever necessary so that it is ready for use by the models I am going to build. The next step is to take advantage for these newly prepared data sources to help me build my suite of reproducible models for analysis regarding the data mining goals.

### Assessment of tools and techniques

I am going to be using multiple tools for this project. 

1. Firstly, I will be using the ProjectTemplate library which allows me to automate multiple parts of the data analysis process such as organizing files, loading packages and data sets into memory and munging and preprocessing my data is a form that renders them ready for analysis([\textcolor{blue}{Git, N/A}](http://projecttemplate.net/getting_started.html)).
2. Another tool that I am going to making use of is Git version control that allows me to save copies of my work in progress in case I need to go back to specific point of my work ([\textcolor{blue}{ProjectTemplate, N/A}](https://git-scm.com/book/en/v2/Getting-Started-About-Version-Control)).
3. Next tool I am going to be using is the dplyr package as this is a tool that is very commonly used for data manipulation and analysis ([\textcolor{blue}{Zev, 2014}](http://zevross.com/blog/2014/03/26/four-reasons-why-you-should-check-out-the-r-package-dplyr-3/)). 
4. Lastly the visualization package called ggplot2 will be used to create plots at a high level of abstraction ([\textcolor{blue}{Tidyverse, N/A}](https://ggplot2.tidyverse.org/)).

---

# Data Understanding

## Introduction

This section seeks to describe the data that have been provided to me for this analysis as part of this project. This process will include describing, exploring and verifying the quality of each data set individually.

## Initial Data Collection

The data was initially stored in a zip folder. This folder includes eight distinct data sets that each recorded an aspects of the course and/or the learner's interaction with the course. Additionally, we observe an number of repetitions in some of these data sets which represent different runs of this course. In total there are seven runs. In runs three to seven we see that all types of data sets are apparent. While on the other, in run one only six are present, with sets labeled as "team-members" and "video-stats" are missing. Moreover, in run two we see that only seven are present, with the set labeled as "video-stats" is missing again.


## Enrolments Dataset

This data set has been recorded for all seven runs

### Describe Data

Enrolments is a data set that holds information about multiple characteristics of the learner such the time they enrolled in the course, the time they finished it, their previous education and much more.

### Explore Data

To further explore the contents of this data set we can see below a few rows with their respective column names.

```{r}
str(cyber.security.7_enrolments, strict.width = "wrap", vec.len=2)
```
One thing that we can immediately see is that there are 13 columns in this set, all of which are of class character. Additionally, there are 2342 rows in this run which indicates that this many students enrolled in this course. By looking at the information held in this data set I believe that I could use it to extract some insights that would help me address my data mining goals. The kinds of questions that I could answer include:

Learner Background:

- Number of people enrolled categorized by age, gender, education, country, employment.

Learner Performance:

- Percentage of people who completed the course

### Verify Data Quality

In this data set we see that some columns have a lot of empty cells or cells that are labeled as unknown or with "--". This may cause some issues later on when we are processing the data and should be taken into consideration.


## Leaving survey response Dataset

This data set has been recorded for all seven runs

### Describe Data

This data set is tracking data about the learners that leave the course. It holds information about when they left, why they left and at which point in their learning.

### Explore Data

To further explore the contents of this data set we can see below a few rows with their respective column names.

```{r}
str(cyber.security.7_leaving.survey.responses, strict.width = "wrap", vec.len=2)
```
In the snippet above we can see that eight metrics were recorded for this data set, four of which are of class character and the other four are Integer. In terms of my data mining goals, this set can help me extract some useful information, such as:

Engagement:

-	Percentages of the leaving reasons.
-	Further investigate the trends among steps (if relevant to the course) at which they left.

### Verify Data Quality

The data set has a many NA values in last four columns. Additionally, we can also see that the column "leaving_reason" has some written errors.


## Weekly sentiment survey Dataset

This data set has been recorded for all seven runs

### Describe Data

This data set hold information about the sentiment of the learners at the end of each week in their curriculum. The recorded data include the week of the survey, the rating given, the time of response and the reason for their rating.

### Explore Data

To further explore the contents of this data set we can see below a few rows with their respective column names.

```{r}
str(cyber.security.7_weekly.sentiment.survey.responses, strict.width = "wrap", vec.len=2)
```
In the snippet above we can see the five metrics that were recorded in total. The columns "id", "week_number" and "rating" are integers, while the other two are characters. I can clearly see that this data set can help my analysis in term of understanding learner engagement. The question that can be asked about this data is:

Learner Engagement:

- How well was each week received by the learners?

### Verify Data Quality

The quality of this data set seems to be fine. The only thing that may be a problem is that not all students have given a reason for their rating. However this is something that can be bypassed if we simply ignore that "reason" column.

## Question response Dataset

This data set has been recorded for all seven runs

### Describe Data

This data set includes information about quizzes that were taken by the students.  

### Explore Data

To further explore the contents of this data set we can see below a few rows with their respective column names.

```{r}
str(cyber.security.7_question.response, strict.width = "wrap", vec.len=2)
```
In total we see that there are ten columns in this set. There are many interesting metrics being recorded in this set such as the type of the question, the week it was about, the step number that the quiz held as part of the curriculum, the time of submission and lastly whether or not the answer give by the student was the correct one or not. In regards to my data mining goals, this data set is very useful as it can provide my analysis with a lot of insight. The information that can be extracted includes:

Learner Performance:

-	Success rate for every quiz.
-	Success rate based on the week.
- Success rate based on the number of questions

### Verify Data Quality

The only issue with that can be immediately spotted with this data set is that the column "cloze_response" is almost completely empty.

## Step activity Dataset

This data set has been recorded for all seven runs

### Describe Data

This data set is holding information about the curriculum of each week.

### Explore Data

To further explore the contents of this data set we can see below a few rows with their respective column names.

```{r}
str(cyber.security.7_step.activity, strict.width = "wrap", vec.len=2)
```
The are six metrics being recorded in this set, of which the two are integers, the other three are characters and there is also one that is a numeric. In this data set I see that there is a great amount of information that can be extracted and used to help me inform my data mining goals. Some of the questions that can be answered include:

Learner Engagement:

-	Percentage of learners that completed each step
-	Percentage of learners that completed each week
- Percentage of learners that completed each type of learning material

### Verify Data Quality

Overall the data set looks to be of good quality. However, I have spotted that the "step" column seems a little bit problematic, so lets investigate a bit further.
```{r}
#plotting the step (which consists of week number followed by a dot and then the step number)
#against the actual recorded step number to see if the data is correct
plot(cyber.security.7_step.activity$step_number,cyber.security.7_step.activity$step, xlab = "Step number", ylab = "Step column")
```
By looking at the plot above we clearly see that the step numbers are not correctly recorded in the "step" column.

## Video stats Dataset

This data set has only been recorded for runs three to seven.

### Describe Data

This is a data set that holds statistical metrics about the video material coverage of the learners.
 
### Explore Data

To further explore the contents of this data set we can see below a few rows with their respective column names.

```{r}
str(cyber.security.7_video.stats, strict.width = "wrap", vec.len=2)
```
In this set we have twenty-eight metrics being recorded in total. Most of these metrics are pre-calculated percentage statistics and are classed are numerical values. While there are also six integer classes that represent amounts of learners. Lastly, there is also one character column for the title of the video. The statistics show us many things such as the percentage of people that watched a certain amount of each video, the countries they watched from and some options they used while watching such as subtitles, hd video quality, etc. This data set can be exploited to answer the following questions:

Engagement

-	Percentage of video watched according to their duration.

Target Audience

-	Percentage of viewers that used mobile, desktop, tv, tablet.
-	Percentage of viewers from each continent.

### Verify Data Quality

This appears to be okay. There are however a few things we may assume about it. First is the duration watched by learners for each video cannot account for people that downloaded that video since however much they watched, they did offline. Second assumption is videos with high percentage of watching are those with at least 95%. The reason behind this is that people may pause and close the video just a few seconds before it ends if there is nothing more to watch.

## archtype survey responses

This data set was recorded for all seven runs.

### Describe Data

This set holds the responses of the learners about how they view themselves in regards to what archetype the feel like.

### Explore Data

To further explore the contents of this data set we can see below a few rows with their respective column names.

```{r}
str(cyber.security.7_archetype.survey.responses, strict.width = "wrap", vec.len=2)
```
There are four columns in this set, threes character and one integer. There seems to be nothing valuable in this set that would help in my analysis.

### Verify Data Quality

The data held in this set seems fine. Only thing to add is that it is empty in runs one and two.

## Team members Dataset

This data set has been recorded for runs two to seven.

### Describe Data

The data stored in this set describe how the staff member teams were set up for the course.

### Explore Data

To further explore the contents of this data set we can see below a few rows with their respective column names.

```{r}
str(cyber.security.7_team.members, strict.width = "wrap", vec.len=2)
```
There are five columns in total that describe the roles of the team members and their personal information. This set also seems like it cannot help me very much in my analysis.

### Verify Data Quality

The personally identifiable pieces of information have been retracted from this data set. Other than that the structural integrity of the data seems alright.

---

# Data Preparation

## Data Selection

The data sources that I am going to include are the ones that can provide me with the most relevant and useful information regarding my business objectives. To extract that information I am going to have to work with a variety of data sources. The data sources which I have found to be the most relevant for informing the engagement, performance and background of the learners are the data sets of enrollments, leaving survey response, weekly sentiment survey, quiz performance, step attendance, video coverage. The only ones that I am going to exclude are the archtype survey responses and team members as they do not hold any data that would help me address my business goals.

## Data preprocessiong and cleaning

### Enrolments Dataset

For this dataset I have created six new types of data sets that hold information about the enrollments and graduations separated by the age, gender, education, employment, country and a general one that included all people. To create this six types of data sets, I have created six functions that take in a specific run's dataset and return a new one for the type of information I need. I have called each of these six functions seven times so that all of the necessary data (new data sets) for all seven runs is automatically created in the munge file for enrollments.
The process of creating the functions for new data sets mainly involved separating the rows and columns of the original data according to the needs of my data mining goals and extracting and calculating only what was deemed useful. However, a big part of the dataset was taken advantage of and only a small amount of information was excluded such as the columns for purchase of statement and the country.

### Question Response Dataset

For question response I created one function in the munge that takes in a data set and carries out all of the necessary preprocessing and clean automatically to return a new clear and concise data set that includes all of the needed data for answering my data mining questions. I called this function seven times in the munge file, one for each run and produced seven new data sets that are ready for analysis.
The function takes care of a variety of things that are required to extract the appropriate information from the original data set. Firstly it creates two separate data frames for the numbers of answers given and the number of answers that were correct along with their respective week and step number. From these two data frames it then creates another data frame of the correct answer percentages for each step and week. Next I iterate through all of the found unique steps (quizzed) and find and store in a vector how many questions there were in each one of those quizzes. Another thing that it takes care of is the issue of the wrongful step values which need to be reformatted. To reformat the step numbers, I multiply the weeks by a hundred and add the the step numbers. Lastly, I return a newly created data frame consisting of all the vectors that are needed from my previously created vectors and data frames.

### Weekly sentiment survey Dataset

For the weekly sentiment dataset I decided to create a function that creates a plot. This function only takes into consideration the qualitative data that the users gave in their survey as a reason for their rating. Essentially the function takes in a vector of character responses, cleans them by getting rid of all the stopwords that are very common and meaningless and then sorts the remaining words according to their frequency. The plot that it returns is a "wordcloud" with the most frequent words in the center with a larger font and the rest of the words around them with a smaller font and placed outwards in the plot according to their frequency levels. This function allows me to then compare the sentiment of the learners across runs.

### Video stats Dataset

This particular data set is actually ready for analysis as it is. It does not require any further cleaning or preprocessing in order to answer the data mining goals set out earlier. It already has pre-calculated values for the percentages for all aspects of the variables I want to investigate.

### Step Activity Dataset

As we saw in the previous chapter in the section of Verifying data quality, this particular data set had an issue with the column labeled as "step". To solve this issue in a reproducible and clean way I created a function that takes in a data set, modifies the step column and return a new clean dataset. The modification is essentially a re-calculation of the step column but this time the weeks are represented by the hundreds and the steps by the rest of the number.
To prepare my data for the analyis I want to do, I created another function for this data set that again takes in a step.activity set and returns a new more concise and processed data frame. This data frame consists of all steps, the number of learner that started each step and the number that finished it, their completion percentage, and lastly the time it took them on average to complete each step. The returned data frame provides me with all of the information I need for my analysis in order to answer my data mining questions. To make things even easier, I called both functions together for all seven runs, to clean and preprocess my data so that it is ready for when I start my analysis.

### Leaving survey responses Dataset

This dataset was in need of some clean before I could extract any data from it. To clean it I had to reformat and recalculate the "leaving_step" column just as before; by representing the weeks with the hundreds and the step number with the rest of the value. This was achieved by creating a function that took in a leaving survey responses data set and return a clean one with the aforementioned changes. The was one more issues with this dataset that I had to clean, that is the "leaving_reason" column which has some typos in a few of the reasons given. To fix this issues, I tried finding each one of these problematic reasons and changing their values to the proper piece of text. However, because the typos are not of UTF-8 type, the R language could not understand them and therefore I could not select them in order to change them. Therefore, to bypass this issue, I added another line of code in my cleaning method where I used the "stri_enc_toutf8" function that replaces non-UTF-8 characters with the "REPLACEMENT CHARACTER". I then when and replaced all my faulty reasons with their now searchable "REPLACEMENT CHARACTERS" for all their non_UTF-8 values and turned them into the correct piece of text for the leaving reasons.
To preprocess my information in order to create new data, I created two functions in total. The first function takes in a leaving survey response data set and finds and returns the number of people that left at each step. The second function again takes in the same type of data set and returns the number of times each leaving reason was given by the learners.