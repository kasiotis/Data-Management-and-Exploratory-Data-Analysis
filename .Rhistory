BreastCancer$Class[BreastCancer$Class=="malignant"] = "1"
#also replacing the levels of the class column so that it only has 1 and 0
levels(BreastCancer$Class)[levels(BreastCancer$Class)=="benign"] = "0"
levels(BreastCancer$Class)[levels(BreastCancer$Class)=="malignant"] = "1"
#Checking the levels that are left in class column
levels(BreastCancer$Class)
#checking to see if there is any other values in the Class column apart from 1 and 0
BreastCancer$Class[BreastCancer$Class != "0" & BreastCancer$Class != "1"]
#iterating through each column (columns 2 to 11) and changing them from a
#factor to a numeric class
for (i in 2:10) {
BreastCancer[,i] = as.numeric(as.character(BreastCancer[,i]))
}
#getting the classes from all the columns to check if my changes are successful
sapply(BreastCancer,class)
#Removing the rows that have "NA" values
BreastCancer = na.omit(BreastCancer)
#checking to see if there are any "NA" values left
BreastCancer[is.na(BreastCancer)=="TRUE"]
#creating numerical and and graphical summaries of the data
par(mfrow=c(2,5))
for (i in 2:10) {
sumar = summary(BreastCancer[,i])
print(sumar)
boxplot(BreastCancer[,i], main=colnames(BreastCancer[,i]))
}
library(bestglm)
#extracting the predictor variables
x1 = scale(as.matrix(BreastCancer[,2:10]))
#extracting the response variable
y = BreastCancer$Class
my.data = data.frame(x1,y)
#conducting best subset selection
bss_fit_AIC = bestglm(my.data, family = binomial, IC="AIC")
bss_fit_BIC = bestglm(my.data, family = binomial, IC="BIC")
#examining the results
bss_fit_AIC$Subsets
bss_fit_BIC$Subsets
#identifying the best fitting models
(best_AIC = bss_fit_AIC$ModelReport$Bestk)
(best_BIC = bss_fit_BIC$ModelReport$Bestk)
#Create multi-panel plotting device
par(mfrow=c(1,2))
# Producing plot, highlighting optimal value of k
plot(0:9, bss_fit_AIC$Subsets$AIC, xlab="Number of predictors",
ylab="AIC", type="b")
points(best_AIC, bss_fit_AIC$Subsets$AIC[best_AIC+1], col="red", pch=16)
# Producing plot, highlighting optimal value of k
plot(0:9, bss_fit_BIC$Subsets$BIC, xlab="Number of predictors",
ylab="BIC", type="b")
points(best_BIC, bss_fit_BIC$Subsets$BIC[best_BIC+1], col="red", pch=16)
pstar = 6
#checking the predictors in the 6-predictor model
bss_fit_AIC$Subsets[pstar+1,]
#creating a reduced dataset containing only the selected predictor
(indices = as.logical(bss_fit_AIC$Subsets[pstar+1, 2:10]))
BreastCancer.red = data.frame(x1[,indices], y)
#getting the regression coefficients for this model
logreg6_fit = glm(y~., data = BreastCancer.red, family = "binomial")
summary(logreg6_fit)
library(glmnet)
grid = 10^seq(-4, -1, length.out=100)
lasso_fit = glmnet(x1, y, family = "binomial", alpha = 1,
standardize = FALSE, lambda = grid)
#examining the effect of the tuning parameter on the parameter estimates
plot(lasso_fit, xvar = "lambda", label = TRUE)
#examining the effect of the tuning parameter on the parameter estimates
plot(lasso_fit, xvar = "lambda", col=rainbow(9), label = TRUE)
#examining the effect of the tuning parameter on the parameter estimates
plot(lasso_fit, xvar = "lambda", col=rainbow(9), label = TRUE)
#choosing a grid of values for the tuning parameter
grid = 10^seq(-4, -1, length.out=1000)
#fitting a model with LASSO penalty for each value of the tuning parameter
lasso_fit = glmnet(x1, y, family = "binomial", alpha = 1,
standardize = FALSE, lambda = grid)
#examining the effect of the tuning parameter on the parameter estimates
plot(lasso_fit, xvar = "lambda", col=rainbow(9), label = TRUE)
#choosing a grid of values for the tuning parameter
grid = 10^seq(-4, -1, length.out=100)
#fitting a model with LASSO penalty for each value of the tuning parameter
lasso_fit = glmnet(x1, y, family = "binomial", alpha = 1,
standardize = FALSE, lambda = grid)
#examining the effect of the tuning parameter on the parameter estimates
plot(lasso_fit, xvar = "lambda", col=rainbow(9), label = TRUE)
#to select a single value for the tuning parameter, I will use cross-validation
lasso_cv_fit = cv.glmnet(x1, y, family="binomial", alpha=1, standardize=FALSE,
lambda = grid, type.measure = "class")
plot(lasso_cv_fit)
#finding the optimal value for the tuning parameter
(lambda_lasso_min = lasso_cv_fit$lambda.min)
which_lambda_lasso = which(lasso_cv_fit$lambda == lambda_lasso_min)
coef(lasso_fit, s=lambda_lasso_min)
which_lambda_lasso
summary(logreg6_fit)
#checking the predictors in the 6-predictor model
bss_fit_AIC$Subsets[pstar+1,]
#Find the parameter estimates associated with optimal value of the tuning parameter
coef(lasso_fit, s=lambda_lasso_min)
#finding the optimal value for the tuning parameter
(lambda_lasso_min = lasso_cv_fit$lambda.min)
which_lambda_lasso = which(lasso_cv_fit$lambda == lambda_lasso_min)
(which_lambda_lasso = which(lasso_cv_fit$lambda == lambda_lasso_min))
#Find the parameter estimates associated with optimal value of the tuning parameter
coef(lasso_fit, s=lambda_lasso_min)
#finding the optimal value for the tuning parameter
(lambda_lasso_min = lasso_cv_fit$lambda.min)
lasso_cv_fit$cvm[i]
#
lasso_cv_fit$cvm[67]
?lm
#visualizing how the test error varies with the tuning parameter
plot(lasso_cv_fit)
?glmnet
#checking the predictors in the 6-predictor model
bss_fit_AIC$Subsets[pstar+1,]
#identifying the best fitting models
(best_AIC = bss_fit_AIC$ModelReport$Bestk)
(best_BIC = bss_fit_BIC$ModelReport$Bestk)
#checking the predictors in the 6-predictor model
bss_fit_AIC$Subsets[pstar+1,]
#checking the predictors in the 6-predictor model
bss_fit_AIC$Subsets[pstar+1,]
#creating a reduced dataset containing only the selected predictors
(indices = as.logical(bss_fit_AIC$Subsets[pstar+1, 2:10]))
View(BreastCancer.red)
#mean MSE
lasso_cv_fit$cvm[which_lambda_lasso]
#Find the parameter estimates associated with optimal value of the tuning parameter
coef(lasso_fit, s=lambda_lasso_min)
View(BreastCancer.red)
View(BreastCancer)
View(x1)
logreg6_train = glm(y~., data = BreastCancer.red[1:455,], family = "binomial")
View(logreg6_train)
phat_test = predict(logreg6_train, BreastCancer.red[456:683,], family="binomial")
yhat_test = ifelse(phat_test > 0.5, 1, 0)
View(x1)
1-mean(y[BreastCancer[456:683]] == yhat_test)
1-mean(y[BreastCancer[456:683,]] == yhat_test)
yhat_test
y
#checking the predictors in the 6-predictor model
bss_fit_AIC$Subsets[pstar+1,]
#checking the predictors in the 6-predictor model
bss_fit_AIC$Subsets[pstar,]
#checking the predictors in the 6-predictor model
bss_fit_AIC$Subsets[pstar+1,]
#checking the predictors in the 6-predictor model
bss_fit_AIC$Subsets
[pstar+1,]
#checking the predictors in the 6-predictor model
bss_fit_AIC$Subsets[pstar+1,]
#creating a reduced dataset containing only the selected predictors
(indices = as.logical(bss_fit_AIC$Subsets[pstar+1, 2:10]))
#creating a reduced dataset containing only the selected predictors
(indices = as.logical(bss_fit_AIC$Subsets[pstar+1, 2:11]))
#creating a reduced dataset containing only the selected predictors
(indices = as.logical(bss_fit_AIC$Subsets[pstar+1, 2:10]))
#
plot(BreastCancer$Cl.thickness, BreastCancer$Cell.size, BreastCancer$Cell.shape,
BreastCancer$Marg.adhesion, BreastCancer$Epith.c.size, BreastCancer$Bare.nuclei,
BreastCancer$Bl.cromatin, BreastCancer$Normal.nucleoli, BreastCancer$Mitoses,
col=BreastCancer$Class, pch=BreastCancer$Class)
#
plot(BreastCancer$Cl.thickness, BreastCancer$Cell.size, BreastCancer$Cell.shape,
BreastCancer$Marg.adhesion, BreastCancer$Epith.c.size, BreastCancer$Bare.nuclei,
BreastCancer$Bl.cromatin, BreastCancer$Normal.nucleoli, BreastCancer$Mitoses,
col=BreastCancer$Class, pch=BreastCancer$Class)
#
plot(BreastCancer$Cl.thickness, BreastCancer$Cell.size, BreastCancer$Cell.shape,
BreastCancer$Marg.adhesion, BreastCancer$Epith.c.size, BreastCancer$Bare.nuclei,
BreastCancer$Bl.cromatin, BreastCancer$Normal.nucleoli, BreastCancer$Mitoses,
col=as.numeric(BreastCancer$Class), pch=BreastCancer$Class)
#
plot(BreastCancer$Cl.thickness, BreastCancer$Cell.size, BreastCancer$Cell.shape,
BreastCancer$Marg.adhesion, BreastCancer$Epith.c.size, BreastCancer$Bare.nuclei,
BreastCancer$Bl.cromatin, BreastCancer$Normal.nucleoli, BreastCancer$Mitoses,
col=as.numeric(BreastCancer$Class), pch=as.numeric(BreastCancer$Class))
library(nclSLR)
lda_fit = linDA(variables = BreastCancer[,2:10], group = BreastCancer$Class)
lda_fit
qda_fit = quaDA(variables = BreastCancer[,2:10], group = BreastCancer$Class)
qda_fit
?qda_fit
??qda_fit
?qdaDA()
?qdaDA
?quaDA
library(mvtnorm)
install.packages(mvtnorm)
install.packages(mvtnorm)
library(mvtnorm)
library(mvtnorm)
install.packages(mvtnorm)
library(mvtnorm)
library(mvtnorm)
dmvnorm(BreastCancer)
dmvnorm(BreastCancer[2:10])
bb = dmvnorm(BreastCancer[2:10])
plot(bb)
barplot(bb)
barplot(sort(bb))
barplot(bb)
lda_fit$functions
str(lda_fit$functions)
lda_fit$functions
lda_fit$functions[,1]
lda_fit$functions[,1][[2]]
lda_fit$functions[,1]
lda_fit$functions[,2]
mean(lda_fit$functions[,2])
mean(lda_fit$functions[1,2])
mean(lda_fit$functions[2:10,2])
benign.mean = mean(lda_fit$functions[2:10,1])
(benign.mean = mean(lda_fit$functions[2:10,1]))
(melignant.mean = mean(lda_fit$functions[2:10,2]))
(benign.mean = mean(lda_fit$functions[,1]))
(melignant.mean = mean(lda_fit$functions[,2]))
(lda.benign.mean = mean(lda_fit$functions[,1]))
(lda.melignant.mean = mean(lda_fit$functions[,2]))
(qda.benign.mean = mean(qda_fit$functions[,1]))
qda_fit$functions
qda_fit$scores
(lda.benign.mean = mean(lda_fit$scores[,1]))
(lda.melignant.mean = mean(lda_fit$functions[,2]))
(lda.benign.mean = mean(lda_fit$scores[,1]))
(lda.melignant.mean = mean(lda_fit$scores[,2]))
(qda.benign.mean = mean(qda_fit$scores[,1]))
(qda.melignant.mean = mean(qda_fit$scores[,2]))
#group means i guess
cat("lda benign group mean: ",lda.benign.mean = mean(lda_fit$scores[,1]))
#fitting the linear discriminant analysis
(lda_fit = linDA(variables = BreastCancer[,2:10], group = BreastCancer$Class)) #2 groups
#group means i guess
cat("lda benign group mean: ",lda.benign.mean = mean(lda_fit$scores[,1]))
cat("lda malignant group mean: ",lda.malignant.mean = mean(lda_fit$scores[,2]))
#fitting the quadratic discriminant analysis
(qda_fit = quaDA(variables = BreastCancer[,2:10], group = BreastCancer$Class)) #2 groups
#group means i guess
cat("qda benign group mean: ",qda.benign.mean = mean(qda_fit$scores[,1]))
cat("qda malignant group mean: ",qda.malignant.mean = mean(qda_fit$scores[,2]))
#identifying the best fitting models
(best_AIC = bss_fit_AIC$ModelReport$Bestk)
(best_BIC = bss_fit_BIC$ModelReport$Bestk)
#Create multi-panel plotting device
par(mfrow=c(1,2))
# Producing plot, highlighting optimal value of k
plot(0:9, bss_fit_AIC$Subsets$AIC, xlab="Number of predictors",
ylab="AIC", type="b")
points(best_AIC, bss_fit_AIC$Subsets$AIC[best_AIC+1], col="red", pch=16)
# Producing plot, highlighting optimal value of k
plot(0:9, bss_fit_BIC$Subsets$BIC, xlab="Number of predictors",
ylab="BIC", type="b")
points(best_BIC, bss_fit_BIC$Subsets$BIC[best_BIC+1], col="red", pch=16)
#checking the predictors in the 6-predictor model
bss_fit_AIC$Subsets[pstar+1,]
#creating a reduced dataset containing only the selected predictors
(indices = as.logical(bss_fit_AIC$Subsets[pstar+1, 2:10]))
View(BreastCancer.red)
#which parameter was the minimum?
which_lambda_lasso = which(lasso_cv_fit$lambda == lambda_lasso_min)
#mean MSE
lasso_cv_fit$cvm[which_lambda_lasso]
nrow(BreastCancer)
#fitting the linear discriminant analysis
(lda_fit = linDA(variables = BreastCancer[,2:10], group = BreastCancer$Class)) #2 groups
#fitting the quadratic discriminant analysis
(qda_fit = quaDA(variables = BreastCancer[,2:10], group = BreastCancer$Class)) #2 groups
#group means i guess
cat("lda benign group mean: ",lda.benign.mean = mean(lda_fit$classification[,1]))
#group means i guess
cat("lda benign group mean: ",lda.benign.mean = mean(lda_fit$scores[,1]))
#fitting the linear discriminant analysis
(lda_fit = linDA(variables = BreastCancer[,2:10], group = BreastCancer$Class)) #2 groups
#group means i guess
cat("lda benign group mean: ",lda.benign.mean = mean(lda_fit$scores[,1]))
cat("lda malignant group mean: ",lda.malignant.mean = mean(lda_fit$scores[,2]))
qda_fit$classification
lda_fit$classification
summary(lda_fit)
(lda_fit = ldA(Class~., data = BreastCancer) #2 groups
(lda_fit = ldA(Class~., data = BreastCancer)) #2 groups
)))))))))))))))))))))))
(lda_fit = ldA(Class~., data = BreastCancer)) #2 groups
(lda_fit = lda(Class~., data = BreastCancer)) #2 groups
library(MASS)
(lda_fit = lda(Class~., data = BreastCancer)) #2 groups
(lda_fit = lda(Class~., data = BreastCancer[2:10])) #2 groups
(lda_fit = lda(Class~., data = BreastCancer[,2:11])) #2 groups
#fitting the quadratic discriminant analysis
(qda_fit = qda(variables = BreastCancer[,2:10], group = BreastCancer$Class)) #2 groups
#fitting the quadratic discriminant analysis
(qda_fit = qda(Class~., data = BreastCancer[,2:10])) #2 groups
#fitting the quadratic discriminant analysis
(qda_fit = qda(Class~., data = BreastCancer[,2:11])) #2 groups
?cv.glmnet
#group means i guess
cat("lda benign group mean: ",lda.benign.mean = mean(lda_fit$means]))
#group means i guess
cat("lda benign group mean: ",lda.benign.mean = mean(lda_fit$means))
lda_fit$means
lda_fit$means[1,]
cat("lda malignant group mean: ",lda.malignant.mean = mean(lda_fit$means[2,]))
lda_fit$means[2,]
View(BreastCancer.red)
reg_cv = function(x1, y, fold_ind){
Xy = data.frame(x1, y=y)
nfolds = max(fold_ind)
if(!all.equal(sort(unique(fold_ind)), 1:nfolds)) stop("Invalid fold partition.")
cv_errors = numeric(nfolds)
for (fold in 1:nfolds) {
tmp_fit = glm(y~., data = Xy[fold_ind!=fold,], family = "binomial")
phat = predict(tmp_fit, Xy[fold_ind == fold,])
yhat = ifelse(phat > 0.5, 1, 0)
yobs = y[fold_ind==fold]
cv_errors[fold] = 1 - mean(y[fold_ind!=fold] == yhat)
}
}
View(BreastCancer.red)
reg_cv(BreastCancer.red[,1:6], y, fold_index)
# 10-fold cross validation
nfolds = 10
fold_index = sample(nfolds, nrow(BreastCancer), replace = TRUE)
reg_cv(BreastCancer.red[,1:6], y, fold_index)
p=reg_cv(BreastCancer.red[,1:6], y, fold_index)
reg_cv = function(x1, y, fold_ind){
Xy = data.frame(x1, y=y)
nfolds = max(fold_ind)
if(!all.equal(sort(unique(fold_ind)), 1:nfolds)) stop("Invalid fold partition.")
cv_errors = numeric(nfolds)
for (fold in 1:nfolds) {
tmp_fit = glm(y~., data = Xy[fold_ind!=fold,], family = "binomial")
phat = predict(tmp_fit, Xy[fold_ind == fold,])
yhat = ifelse(phat > 0.5, 1, 0)
yobs = y[fold_ind==fold]
cv_errors[fold] = 1 - mean(y[fold_ind!=fold] == yhat)
}
fold_sizes = numeric(nfolds)
for (fold in 1:nfold) fold_sizes[fold] = length(which(fold_ind==fold))
test_error = weighted.mean(cv_errors, w=fold_sizes)
return(test_error)
}
p=reg_cv(BreastCancer.red[,1:6], y, fold_index)
s
# 10-fold cross validation
nfolds = 10
reg_cv = function(x1, y, fold_ind){
Xy = data.frame(x1, y=y)
nfolds = max(fold_ind)
if(!all.equal(sort(unique(fold_ind)), 1:nfolds)) stop("Invalid fold partition.")
cv_errors = numeric(nfolds)
for (fold in 1:nfolds) {
tmp_fit = glm(y~., data = Xy[fold_ind!=fold,], family = "binomial")
phat = predict(tmp_fit, Xy[fold_ind == fold,])
yhat = ifelse(phat > 0.5, 1, 0)
yobs = y[fold_ind==fold]
cv_errors[fold] = 1 - mean(y[fold_ind!=fold] == yhat)
}
fold_sizes = numeric(nfolds)
for (fold in 1:nfolds) fold_sizes[fold] = length(which(fold_ind==fold))
test_error = weighted.mean(cv_errors, w=fold_sizes)
return(test_error)
}
p=reg_cv(BreastCancer.red[,1:6], y, fold_index)
p
min(p)
reg_cv = function(x1, y, fold_ind){
Xy = data.frame(x1, y=y)
nfolds = max(fold_ind)
if(!all.equal(sort(unique(fold_ind)), 1:nfolds)) stop("Invalid fold partition.")
cv_errors = numeric(nfolds)
for (fold in 1:nfolds) {
tmp_fit = glm(y~., data = Xy[fold_ind!=fold,], family = "binomial")
phat = predict(tmp_fit, Xy[fold_ind == fold,], type = "response")
yhat = ifelse(phat > 0.5, 1, 0)
yobs = y[fold_ind==fold]
cv_errors[fold] = 1 - mean(yobs == yhat)
}
fold_sizes = numeric(nfolds)
for (fold in 1:nfolds) fold_sizes[fold] = length(which(fold_ind==fold))
test_error = weighted.mean(cv_errors, w=fold_sizes)
return(test_error)
}
p=reg_cv(BreastCancer.red[,1:6], y, fold_index)
p
View(BreastCancer)
View(BreastCancer.red)
#identifying the best fitting models
(best_AIC = bss_fit_AIC$ModelReport$Bestk)
(best_BIC = bss_fit_BIC$ModelReport$Bestk)
bss_fit_BIC$Subsets[pstar+1]
bss_fit_BIC$Subsets[pstar+1,]
#examining the results
bss_fit_AIC$Subsets
bss_fit_BIC$Subsets
lda_fit$means
#group means i guess
cat("lda benign group mean: ",lda.benign.mean = mean(lda_fit$means[1,]))
#group means i guess
cat("lda benign group mean: ",lda.benign.mean = lda_fit$means[1,])
lda_fit$means
(lda_fit = lda(Class~., data = BreastCancer[,2:11])) #2 groups
lda_fit$means
lda_fit$means
qda_fit$means
reg_cv_lda = function(x1, y, fold_ind){
Xy = data.frame(x1, y=y)
nfolds = max(fold_ind)
if(!all.equal(sort(unique(fold_ind)), 1:nfolds)) stop("Invalid fold partition.")
cv_errors = numeric(nfolds)
for (fold in 1:nfolds) {
tmp_fit = lda(y~., data = Xy[fold_ind!=fold,])
phat = predict(tmp_fit, Xy[fold_ind == fold,])
yhat = phat$
yobs = y[fold_ind==fold]
cv_errors[fold] = 1 - mean(yobs == yhat)
}
fold_sizes = numeric(nfolds)
for (fold in 1:nfolds) fold_sizes[fold] = length(which(fold_ind==fold))
test_error = weighted.mean(cv_errors, w=fold_sizes)
return(test_error)
}
reg_cv_lda(x1[,2:10], y, fold_index)
View(x1)
reg_cv_lda(x1, y, fold_index)
reg_cv_lda = function(x1, y, fold_ind){
Xy = data.frame(x1, y=y)
nfolds = max(fold_ind)
if(!all.equal(sort(unique(fold_ind)), 1:nfolds)) stop("Invalid fold partition.")
cv_errors = numeric(nfolds)
for (fold in 1:nfolds) {
tmp_fit = lda(y~., data = Xy[fold_ind!=fold,])
phat = predict(tmp_fit, Xy[fold_ind == fold,])
yhat = phat$class
yobs = y[fold_ind==fold]
cv_errors[fold] = 1 - mean(yobs == yhat)
}
fold_sizes = numeric(nfolds)
for (fold in 1:nfolds) fold_sizes[fold] = length(which(fold_ind==fold))
test_error = weighted.mean(cv_errors, w=fold_sizes)
return(test_error)
}
reg_cv_lda(x1, y, fold_index)
reg_cv_qda(x1, y, fold_index)
reg_cv_qda = function(x1, y, fold_ind){
Xy = data.frame(x1, y=y)
nfolds = max(fold_ind)
if(!all.equal(sort(unique(fold_ind)), 1:nfolds)) stop("Invalid fold partition.")
cv_errors = numeric(nfolds)
for (fold in 1:nfolds) {
tmp_fit = qda(y~., data = Xy[fold_ind!=fold,])
phat = predict(tmp_fit, Xy[fold_ind == fold,])
yhat = phat$class
yobs = y[fold_ind==fold]
cv_errors[fold] = 1 - mean(yobs == yhat)
}
fold_sizes = numeric(nfolds)
for (fold in 1:nfolds) fold_sizes[fold] = length(which(fold_ind==fold))
test_error = weighted.mean(cv_errors, w=fold_sizes)
return(test_error)
}
reg_cv_qda(x1, y, fold_index)
reg_cv_lasso = cv.glmnet(x1, y, family="binomial", alpha=1, standardize=FALSE,
lambda = grid, type.measure = "class", foldid = fold_index)
reg_cv_lasso$lambda.min
reg_cv_lasso = cv.glmnet(x1, y, family="binomial", alpha=1, standardize=FALSE,
lambda = grid, type.measure = "class", foldid = fold_index)
#finding the optimal value for the tuning parameter
(lambda_lasso_min = reg_cv_lasso$lambda.min)
#which parameter was the minimum?
which_lambda_lasso = which(reg_cv_lasso$lambda == lambda_lasso_min)
#test error
reg_cv_lasso$cvm[which_lambda_lasso]
reg_cv_qda(x1, y, fold_index)
#test error
reg_cv_lasso$cvm[which_lambda_lasso]
#test error
reg_cv_lasso$cvm[which_lambda_lasso]
reg_cv_lasso = cv.glmnet(x1, y, family="binomial", alpha=1, standardize=FALSE,
lambda = grid, type.measure = "class",
nfolds = nfolds, foldid = fold_index)
#finding the optimal value for the tuning parameter
(lambda_lasso_min = reg_cv_lasso$lambda.min)
#which parameter was the minimum?
which_lambda_lasso = which(reg_cv_lasso$lambda == lambda_lasso_min)
#test error
reg_cv_lasso$cvm[which_lambda_lasso]
setwd("C:/Users/NIKI IOANNOU/Google Drive/University Material/Master's Degree/CSC8631 Data Management and EDA/Assignments/Source Code/template")
#load projTemp library
library(ProjectTemplate)
load.project()
load.project()
leaving.step.merged.plot()
leaving.step.merged.plot = function(){
#plotting the average number of people that have left at each step (merged dataset version of runs 4-7)
barplot(leaving.steps.data.merged$frequency, axisnames = TRUE, names.arg = leaving.steps.data.merged$leaving.step, col = 1:42, axis.lty = 1,
xlab = "Leaving Step", ylab = "Frequency", ylim = c(0,max(leaving.steps.data.merged$frequency)*1.1), cex.axis=0.7, cex.names=0.50,
main = "Average number of people who left (Runs 4-7 merged)")
}
leaving.step.merged.plot()
leaving.reason.merged.plot = function(){
#plotting the average number of people that have left for each reason (merged dataset version of runs 4-7)
barplot(leaving.reason.data.merged$frequency, axisnames = TRUE, names.arg = leaving.reason.data.merged$leaving.reason, col = 1:6,
cex.axis=0.7, cex.names=0.50, ylim = c(0,max(leaving.reason.data.merged$frequency)*1.2), xlab = "Leaving Reasons", ylab = "Frequency",
main = "Average number of people who left (Runs 4-7 merged)")
}
leaving.reason.merged.plot()
#wordcloud for week 1
wordcloud.run6 = function(){
get.weekly.sentiment.plot(cyber.security.6_weekly.sentiment.survey.responses$reason)
}
wordcloud.run6()
